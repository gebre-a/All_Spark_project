{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a2adec-fb86-443f-ba86-aa225059569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Step 1: Extract\n",
    "def extract(file_path):\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"ETL Example\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/bank_db.branch_transactions\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/bank_db.branch_transactions\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    return df\n",
    "\n",
    "# Step 2: Transform\n",
    "def transform(df):\n",
    "    # Example transformation: increase salary by 10%\n",
    "    transformed_df = df.withColumn(\"salary\", col(\"salary\") * 1.1)\n",
    "    return transformed_df\n",
    "\n",
    "# Step 3: Load\n",
    "def load(df):\n",
    "    # Write to MongoDB\n",
    "    # df.write.format(\"mongo\").mode(\"overwrite\").save()\n",
    "    df.write \\\n",
    "    .format(\"mongo\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"uri\", \"mongodb://localhost:27017/bank_db.branch_transactions\") \\\n",
    "    .save()\n",
    "\n",
    "def main():\n",
    "    input_path = r'C:\\Users\\kidan\\OneDrive\\Desktop\\data scince jupyter\\Salary_Data.csv'\n",
    "\n",
    "    # ETL Process\n",
    "    data = extract(input_path)\n",
    "    transformed_data = transform(data)\n",
    "    load(transformed_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018d0ac2-5e72-4a01-adb7-b44527872500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+------------------+\n",
      "|YearsExperience|                 _id|            salary|\n",
      "+---------------+--------------------+------------------+\n",
      "|            1.1|{671a000669bcb764...|           43277.3|\n",
      "|            1.3|{671a000669bcb764...| 50825.50000000001|\n",
      "|            1.5|{671a000669bcb764...|41504.100000000006|\n",
      "|            2.0|{671a000669bcb764...| 47877.50000000001|\n",
      "|            2.2|{671a000669bcb764...|43880.100000000006|\n",
      "|            2.9|{671a000669bcb764...|62306.200000000004|\n",
      "|            3.0|{671a000669bcb764...|           66165.0|\n",
      "|            3.2|{671a000669bcb764...| 59889.50000000001|\n",
      "|            3.2|{671a000669bcb764...|           70889.5|\n",
      "|            3.7|{671a000669bcb764...|           62907.9|\n",
      "|            3.9|{671a000669bcb764...|           69539.8|\n",
      "|            4.0|{671a000669bcb764...|           61373.4|\n",
      "|            4.0|{671a000669bcb764...|62652.700000000004|\n",
      "|            4.1|{671a000669bcb764...|62789.100000000006|\n",
      "|            4.5|{671a000669bcb764...|           67222.1|\n",
      "|            4.9|{671a000669bcb764...|           74731.8|\n",
      "|            5.1|{671a000669bcb764...| 72631.90000000001|\n",
      "|            5.3|{671a000669bcb764...|           91396.8|\n",
      "|            5.9|{671a000669bcb764...|           89499.3|\n",
      "|            6.0|{671a000669bcb764...|103334.00000000001|\n",
      "+---------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Step 1: Extract\n",
    "def extract(file_path):\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"ETL Example\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/bank_db.branch_transactions\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/bank_db.branch_transactions\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    return df\n",
    "\n",
    "# Step 2: Transform\n",
    "def transform(df):\n",
    "    # Example transformation: increase salary by 10%\n",
    "    transformed_df = df.withColumn(\"salary\", col(\"salary\") * 1.1)\n",
    "    return transformed_df\n",
    "\n",
    "# Step 3: Load\n",
    "def load(df):\n",
    "    # Write to MongoDB\n",
    "    df.write \\\n",
    "    .format(\"mongo\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"uri\", \"mongodb://localhost:27017/bank_db.branch_transactions\") \\\n",
    "    .save()\n",
    "\n",
    "# Step 4: Read from MongoDB\n",
    "def read_from_mongodb(spark):\n",
    "    # Read data from MongoDB\n",
    "    mongodb_df = spark.read.format(\"mongo\").load()\n",
    "    return mongodb_df\n",
    "\n",
    "def main():\n",
    "    input_path = r'C:\\Users\\kidan\\OneDrive\\Desktop\\data scince jupyter\\Salary_Data.csv'\n",
    "\n",
    "    # ETL Process\n",
    "    data = extract(input_path)\n",
    "    transformed_data = transform(data)\n",
    "    load(transformed_data)\n",
    "\n",
    "    # Read from MongoDB\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Read from MongoDB\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/bank_db.branch_transactions\") \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "    loaded_data = read_from_mongodb(spark)\n",
    "    \n",
    "    # Show the data read from MongoDB\n",
    "    loaded_data.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198512d6-efaf-426b-93b3-527706fd6e82",
   "metadata": {},
   "source": [
    "# Read(extract) data from mogodb Transform an load to MongoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bcd8cfc-1165-4cba-9f0d-4e83267df3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Step 1: Extract\n",
    "def extract(spark):\n",
    "    # Read data from MongoDB\n",
    "    df = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://localhost:27017/bank_data.transactions\").load()\n",
    "    return df\n",
    "# Step 2: Transform\n",
    "def transform(df):\n",
    "    # Example transformation: categorize accounts based on amount\n",
    "    transformed_df = df.withColumn(\n",
    "        \"account_category\", \n",
    "        when(col(\"amount\") < 50000, \"Legal\")\n",
    "        .when(col(\"amount\") >= 50000, \"Saspeciese\")\n",
    "        .otherwise(\"Unknown\")\n",
    "    )\n",
    "    return transformed_df\n",
    "\n",
    "# Step 3: Load\n",
    "def load(df):\n",
    "    # Write the transformed data back to MongoDB\n",
    "    df.write.format(\"mongo\").mode(\"overwrite\").option(\"uri\", \"mongodb://localhost:27017/bank_data.branch_transactions\").save()\n",
    "\n",
    "def main():\n",
    "    # Create Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"ETL Bank Data Example\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/bank_data.transactions\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/bank_data.branch_transactions\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # ETL Process\n",
    "    extracted_data = extract(spark)\n",
    "    transformed_data = transform(extracted_data)\n",
    "    load(transformed_data)\n",
    "\n",
    "    # Optional: Read back the transformed data to verify\n",
    "    loaded_data = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://localhost:27017/bank_data.branch_transaction\").load()\n",
    "    print(loaded_data.show())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8409c9b1-c5a7-4fb2-99d6-ce4665c6a064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
